---
title: "Fitting Exercise Week 8"
author: "Cora Hirst"
execute: 
  warning: false
  message: false
---

```{r setup}
#install.packages("tidymodels")
library(tidymodels)

#install.packages("tidyverse")
library(tidyverse)

#install.packages("ggplot2")
library(ggplot2)

#install.packages("here")
library(here)

#define seed
rng.seed = 1234
```

# More Processing

The code chunk below loads the cleaned data from `fitting-exercise\fitting_exercise.qmd`.

```{r load-data}
#loading data from cleaned data rds
mavoglurant_data_cleaned = readRDS(here("ml-models-exercise", "data", "mavoglurant_data_cleaned.rds"))

```


The following code chunk combines `RACE` categories $7$ and $88$ into a single category, called $3$. 

```{r combine-RACE-categories}
#combines RACE categories 7 and 88 into new category, 3
mavoglurant_data1 = mavoglurant_data_cleaned %>%
  mutate(RACE = case_when(RACE %in% c(7,88) ~ factor(3), .default = RACE)) # %in% is essentialy == 7 | == 88

```

# Pairwise Correlation
 
We would like to remove continuous variables that are correlated with one another, as to not bias our learned models by weighing one variable more heavily due to its correlation with another. 
The following code chunk generates a pairwise correlation plot for the continuous variables, `y`, `AGE`, `WT`, and `HT`:

```{r correlation-plot}
# subset data for continuous variables 
mavo_cont = mavoglurant_data1 %>% select(y, AGE, WT, HT)

# generate correlation matrix
correlation_matrix = cor(mavo_cont, use = "pairwise.complete.obs")

## plot correlation matrix as map 
cont.corrplot = corrplot::corrplot(correlation_matrix, method = "color", type = "upper")

```

The Height and weight variables are most strongly correlated, with a coefficient around `r signif(correlation_matrix["WT", "HT"], 1)`. Age and height are the next most correlated, with a significantly looser correlation coefficient of `r signif(correlation_matrix["AGE", "HT"], 1)`. IF we consider an excessive correlation threshold of 0.9, we need not be concerned that extreme colinearity between our continuous variables will bias the weight assigned each variable of our predictive models. 

# Feature Engineering

Even though the correlation between `HT` and `WT` (height and weight) here isn't super high, we still expect there to be some colinearity between them. As a result, we'd like to create a new feature that incorporates both of these variables - a new variable, `BMI`.

While we are unsure of the units of `HT` and `WT` - unfortunately, there's little documentation available to tell us - we can guess from the range of weights (56.6 - 115.3) and heights (1.5 - 1.93) that `WT` is given in units of kgs and `HT` is given in units of meters. 

We thus use the following equation 

$$
BMI = \frac{WT}{HT^2} kg/m^2
$$

to compute the BMI from the height and weight recorded for each observation in the code chunk below: 

```{r compute-BMI}
# calculating BMI and storing as new feature in `mavoglurant_data1` object
mavoglurant_data1 = mavoglurant_data1 %>%
  mutate(BMI = WT/(HT^2))

#lets take a look at our BMIs 

ggplot() + geom_boxplot(data = mavoglurant_data1, aes(x = BMI))
```
This distribution of BMI around an average of a little under 27 is similar to what we'd expect for North America (add CDC reference later.)

# Model Building

In this section, we will be fitting three models: 

- For the first model, we’ll revisit the one we had previously, namely a linear model with all predictors.

- For our second model, we’ll use LASSO regression.

- For our third model, we’ll do a random forest (RF).

Note that we will not be doing a training and test split to test the performance of our model. Instead, we will be fitting the entire dataset and performing cross-validation to observe how the model structure itself (not just the parameterization) performs on the data. 




