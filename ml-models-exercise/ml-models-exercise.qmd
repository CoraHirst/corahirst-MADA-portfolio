---
title: "Fitting Exercise Week 8"
author: "Cora Hirst"
execute: 
  warning: false
  message: false
---

```{r setup}
#install.packages("tidymodels")
library(tidymodels)

#install.packages("tidyverse")
library(tidyverse)

#install.packages("ggplot2")
library(ggplot2)

#install.packages("here")
library(here)

#install.packages("glmnet")
library(glmnet)

#install.packages("ranger")
library(ranger)

#install.packages("doParallel")
library(doParallel)

#define seed
rng.seed = 1234
```

# More Processing

The code chunk below loads the cleaned data from `fitting-exercise\fitting_exercise.qmd`.

```{r load-data}
#loading data from cleaned data rds
mavoglurant_data_cleaned = readRDS(here("ml-models-exercise", "data", "mavoglurant_data_cleaned.rds"))

```


The following code chunk combines `RACE` categories $7$ and $88$ into a single category, called $3$. 

```{r combine-RACE-categories}
#combines RACE categories 7 and 88 into new category, 3
mavoglurant_data1 = mavoglurant_data_cleaned %>%
  mutate(RACE = case_when(RACE %in% c(7,88) ~ factor(3), .default = RACE)) # %in% is essentialy == 7 | == 88

```

# Pairwise Correlation
 
We would like to remove continuous variables that are correlated with one another, as to not bias our learned models by weighing one variable more heavily due to its correlation with another. 
The following code chunk generates a pairwise correlation plot for the continuous variables, `y`, `AGE`, `WT`, and `HT`:

```{r correlation-plot}
# subset data for continuous variables 
mavo_cont = mavoglurant_data1 %>% select(y, AGE, WT, HT)

# generate correlation matrix
correlation_matrix = cor(mavo_cont, use = "pairwise.complete.obs")

## plot correlation matrix as map 
cont.corrplot = corrplot::corrplot(correlation_matrix, method = "color", type = "upper")

```

The Height and weight variables are most strongly correlated, with a coefficient around `r signif(correlation_matrix["WT", "HT"], 1)`. Age and height are the next most correlated, with a significantly looser correlation coefficient of `r signif(correlation_matrix["AGE", "HT"], 1)`. IF we consider an excessive correlation threshold of 0.9, we need not be concerned that extreme colinearity between our continuous variables will bias the weight assigned each variable of our predictive models. 

# Feature Engineering

Even though the correlation between `HT` and `WT` (height and weight) here isn't super high, we still expect there to be some colinearity between them. As a result, we'd like to create a new feature that incorporates both of these variables - a new variable, `BMI`.

While we are unsure of the units of `HT` and `WT` - unfortunately, there's little documentation available to tell us - we can guess from the range of weights (56.6 - 115.3) and heights (1.5 - 1.93) that `WT` is given in units of kgs and `HT` is given in units of meters. 

We thus use the following equation 

$$
BMI = \frac{WT}{HT^2} kg/m^2
$$

to compute the BMI from the height and weight recorded for each observation in the code chunk below: 

```{r compute-BMI}
# calculating BMI and storing as new feature in `mavoglurant_data1` object
mavoglurant_data1 = mavoglurant_data1 %>%
  mutate(BMI = WT/(HT^2))

#lets take a look at our BMIs 

ggplot() + geom_boxplot(data = mavoglurant_data1, aes(x = BMI))
```
This distribution of BMI around an average of a little under 27 is similar to what we'd expect for North America (add CDC reference later.)

# Model Building

In this section, we will be fitting three models: 

- For the first model, we’ll fit a linear model with all predictor,

- for our second model, we’ll use LASSO regression, and

- for our third model, we’ll do a random forest (RF).

Note that we will not be doing a training and test split to test the performance of our model. Instead, we will be fitting the entire dataset and performing cross-validation to observe how the model structure itself (not just the parameterization) performs on the data. 

## First fit: all data = training data

We will be fitting the `y` outcome to all of our predictor variables using a multiple linear regression model, not considering any interaction between the variables, in the code chunk below: 

```{r recipes-and-workflows}
## recipes
rec_y_all = 
  recipes::recipe(y ~ ., 
         data = mavoglurant_data1) %>%
         step_normalize(all_numeric_predictors()) %>% 
         step_dummy(all_nominal_predictors())

## models
# Model 1: choose linear reg model with parsnip package
linear_reg = 
  parsnip::linear_reg() %>% #linear regression model
  parsnip::set_engine("lm") %>% #with lm method
  parsnip::set_mode("regression") #mode = regression, mode outcome is numeric

# Model 2: choose lasso model
lasso_mod = 
  parsnip::linear_reg(penalty = 0.1, mixture = 1) %>%
  parsnip::set_mode("regression") %>%
  parsnip::set_engine("glmnet")

# Model 3: random forest
rf_mod = 
  parsnip::rand_forest() %>%
  parsnip::set_mode("regression") %>%
  parsnip::set_engine("ranger", seed = rng.seed)

## Workflows
# Linear regression 
y_multiple_reg_WF = 
  workflows::workflow() %>% #defining workflow object
  workflows::add_model(linear_reg) %>% #choosing the model for the workflow
  workflows::add_recipe(rec_y_all) #adding the recipe - which variables for outcome, predictor, any interactions.

# Lasso regression
y_multiple_LASSO_WF = 
  workflows::workflow() %>%
  workflows::add_model(lasso_mod) %>%
  workflows::add_recipe(rec_y_all)

# wf regression
y_multiple_rf_WF = 
  workflows::workflow() %>%
  workflows::add_model(rf_mod) %>%
  workflows::add_recipe(rec_y_all)

```

```{r linear-model-y-all}
# Model 1: linear regression with multiple predictors
# fit the model with the defined workflow
model1 = y_multiple_reg_WF %>% fit(mavoglurant_data1) 
# tidy output 
tidy(model1) #print result
```

We have already compared this model to the null model (`fitting-exercise/fitting-exercise.qmd`), but in general, it is important that we do so to compare the performance of our multi-variable linear model. 

The next code chunk fits a LASSO regression to predict `y` from all other variables in the dataset: 

```{r LASSO-model-y-all}
# Model 2: lasso model with multiple predictors
model2 = y_multiple_LASSO_WF %>% fit(mavoglurant_data1)
# tidy output
tidy(model2) #print result
```

And here, we fit a random forest model: 

```{r RF-model-y-all}
# Model 3: random forest model with multiple predictors
model3 = y_multiple_rf_WF %>% fit(mavoglurant_data1)
# summarise output
summary(model3)
```

Here, we would like to determine the RMSE of each model, and plot the predicted values of `y` per observation against the true observations.

```{r model-performance-initial}
### create a data frame of y predictions by model
predictions = data.frame(true = mavoglurant_data1$y,
                         model.1 = predict(model1, new_data = mavoglurant_data1) %>% select(.pred), 
                         model.2 = predict(model2, new_data = mavoglurant_data1) %>% select(.pred),
                         model.3 = predict(model3, new_data = mavoglurant_data1) %>% select(.pred)) 
colnames(predictions) = c("true", "model.1", "model.2", "model.3")

# make it ~long form~
predictions_df = predictions %>%
  pivot_longer(cols = c("model.1", "model.2", "model.3"),
               values_to = "Prediction",
               names_to = "Model") 

### create a data frame of RMSE by model
RMSE_by_model = data.frame(model = c("model.1", "model.2", "model.3"),
                           rmse = c(rmse(data = predictions, truth = true, estimate = model.1)$.estimate,
                                    rmse(data = predictions, truth = true, estimate = model.2)$.estimate,
                                    rmse(data = predictions, truth = true, estimate = model.3)$.estimate))

### plot
plot = ggplot() + geom_point(data = predictions_df, aes(x = true, y = Prediction, col = Model)) + 
  geom_abline(linetype = "dashed") +
  annotate(geom = "text", x = 4500, y = c(2000, 1725, 1500), label = paste0(c("model 1 rmse = ", "model 2 rmse = ", "model 3 rmse = "), as.character(RMSE_by_model$rmse))) + 
  labs(x = "Observed y", y = "Predicted y", "Model Predictions of Training Data vs True Outcome of Training Data")


plot

```

## Tuning to training data without cross validation

This is a silly thing to do, because we will be tuning our parameters to data and testing our model performcance on the same data - which will, of course, lead us to overfit our model. 

But we should try anyway, so that we can see what do expect from overfitting.

### Lasso Model2

```{r tuning-LASSO-prep}
# defining recipe again in the same code chunk
rec_y_all = 
  recipes::recipe(y ~ ., 
         data = mavoglurant_data1) %>%
         step_normalize(all_numeric_predictors()) %>% 
         step_dummy(all_nominal_predictors())

# updating LASSO model to include penalty
lasso_mod2 = linear_reg(penalty = tune(), mixture = 1) %>% 
             set_mode("regression") %>% 
             set_engine("glmnet") 

# updating workflow with new model and recipe
#Lasso regression
y_multiple_LASSO_tune_WF = 
  workflows::workflow() %>%
  workflows::add_model(lasso_mod2) %>%
  workflows::add_recipe(rec_y_tune)

# define paramater grid - vector here because only one tuning parameter
lambda_grid <- grid_regular(penalty(range = c(-5, 2), trans = transform_log10()), levels = 50)

# creating resamples object 
samples_Ltune = apparent(mavoglurant_data1)

```

```{r tuning-LASS0}
# parallel processing is speedy
doParallel::registerDoParallel()

# setting seed
set.seed(rng.seed)

# tuning model
tuned_model2 = lasso_grid <- tune_grid(
              y_multiple_LASSO_tune_WF,
              resamples = samples_Ltune,
              grid = lambda_grid
)

# observe the tuning process
tuned_model2 %>% 
  autoplot()
```

### Random Forest Model

```{r}

# update workflow accordingly 
rf_mod2 <- rand_forest(mode = "regression",
                      mtry = tune(),  # allow for tuning
                      min_n = tune(), # allow for tuning
                      trees = 300) %>% 
           set_engine("ranger", seed = rngseed) # set seed for reproducibility internally

workflow2 <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(rf_mod2) # use updated model

# define a 7 by 7 parameter grid
model3_grid <- grid_regular(mtry(range = c(1,7)),
                            min_n(range = c(1,21)),
                            levels = 7)

# tune the model 
 tuned_model3 <- workflow2 %>% 
               tune_grid(resamples = apparent_resamples,
                        grid = model3_grid,
                         metrics = metric_set(rmse))

# observe the tuning process 
 rf_tune <- tuned_model3 %>% 
              autoplot()

```





